{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "In this unit we will learn about **Convolutional Neural Networks (CNNs)**, which are specifically designed for computer vision.\n",
    "\n",
    "Computer vision is different from generic classification, because when we are trying to find a certain object in the picture, we are scanning the image looking for some specific **patterns** and their combinations. For example, when looking for a cat, we first may look for horizontal lines, which can form whiskers, and then certain combination of whiskers can tell us that it is actually a picture of a cat. Relative position and presence of certain patterns is important, and not their exact position on the image. \n",
    "\n",
    "To extract patterns, we will use the notion of **convolutional filters**. But first, let us load all dependencies and functions that we have defined in the previous units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.2.2->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchvision) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchinfo in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytorchcv in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.0.67)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (from pytorchcv) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from pytorchcv) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->pytorchcv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->pytorchcv) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->pytorchcv) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->pytorchcv) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 및 pytorch.py 파일 함수 로드\n",
    "#!wget https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/pytorchcv.py\n",
    "%pip install torchvision\n",
    "%pip install torchinfo\n",
    "%pip install pytorchcv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pytorchcv\n",
    "# 데이터 분활을 위한 라이브러리 추가\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorchcv import load_fashion_mnist, train, plot_results, plot_convolution, display_dataset\n",
    "load_fashion_mnist(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional filters\n",
    "\n",
    "Convolutional filters are small windows that run over each pixel of the image and compute weighted average of the neighboring pixels.\n",
    "\n",
    "\n",
    "\n",
    "They are defined by matrices of weight coefficients. Let's see the examples of applying two different convolutional filters over our MNIST handwritten digits.\n",
    "\n",
    "The vertical edge filter emphasizes changes in intensity that occur vertically across the image, making it useful for detecting vertical lines and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_convolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVertical edge filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 수직선 판별하기\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plot_convolution(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m],[\u001b[38;5;241m0.\u001b[39m,\u001b[38;5;241m0.\u001b[39m,\u001b[38;5;241m0.\u001b[39m],[\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m1.\u001b[39m]]),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHorizontal edge filter\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 수평선 판별하기\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/introai202401-midterm-2wnsqo/pytorchcv.py:137\u001b[0m, in \u001b[0;36mplot_convolution\u001b[0;34m(t, title)\u001b[0m\n\u001b[1;32m    135\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(title,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m) \u001b[38;5;66;03m# 전체 그래프의 제목을 설정하는데 폰트 크기는 16\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m): \u001b[38;5;66;03m# 5번 반복하여 첫 번째 5개의 이미지에 대해 다음 작업을 수행\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mdata_train\u001b[49m[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# 학습 데이터셋에서 i번째 이미지를 호출\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     ax[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mimshow(im[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# 첫 번째 행의 i번째 서브플롯에 원본 이미지를 표시\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     ax[\u001b[38;5;241m1\u001b[39m][i]\u001b[38;5;241m.\u001b[39mimshow(c(im\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# 두 번째 행의 i번째 서브플롯에 컨볼루션 결과를 표시\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqoAAAEwCAYAAAB/kFPSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxiElEQVR4nO3dfXRU5bn38d8kMDNQzSAGEyJJEBFQBKORcEJLI0dqFLRotQKlEEVEjtqinMohVYy0daEetUspruN5SWIXrCKc8rIUBGl4sxDFE0ExgSryqpKRoEwAIdjkfv7wyZQxE5idzEz2Tr6fteaPuefee1+5f9nbi8nM1mWMMQIAAABsJqGtCwAAAADCoVEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBRxq0qRJcrlcGjduXETzf//738vlcumKK66IcWXndt1118nlcmnDhg1tcvy77rpLLpdLpaWlbXJ8u9ZyNlVVVbr11lt10UUXKTExUS6XS0888YSk5vN84oknQuYBgFU0qoBD3XPPPZKk5cuX66uvvjrn/JKSkpDtYoXmpP05ceKERo8erRUrVigzM1M/+9nPVFBQoKysrBbtb8OGDXK5XLruuuuiWieA9qdTWxcAoGV++MMfqm/fvtq9e7cWLlyoBx98sNm57777rnbs2KHOnTtr4sSJcawyvD/+8Y/6+uuvlZGR0dalIALvvvuu9u3bp2HDhmnz5s1NXidPALHCO6qAQ7lcLk2ePFnSP94tbU7j6zfffLMuuuiimNd2LhkZGRowYIC6du3a1qUgAgcOHJAkXXbZZWFfJ08AsUKjCjjYXXfdpcTERL333nv64IMPws45deqU/vSnP0lq+mf/iooKTZgwQRkZGfJ4POrevbvy8/O1atWqsPvq3bu3XC6X9u3bpxUrVuif//mf1b179+DnE10ul+bMmSNJmjNnjlwuV/Bx1113Bfdzrs+orlu3Tj/96U/Vq1cveTwe9ejRQ0OGDFFRUZGOHDkSnPfNN99owYIFmjBhggYMGKCkpCR16dJF/fv31y9/+Ut9/vnnkS5lxD766CPdd999uvTSS+X1euXz+fTDH/5QCxYsaHabL7/8Ug899JAyMzPl8XiUkZGhBx98UF9++eVZj3XixAnNnj1bl112mTwej9LS0jR58mR99tln5/yIhdVsw2nMtKCgQJL0yiuvhGTayMpnjq+77jqNGDFCkrRx48aQ/fXu3bvJ/LKyMv3kJz9Rz5495Xa7ddFFF+m2225TeXl52P2fWVtJSYlyc3Pl8/mCv7cAnIU//QMO1rNnT40aNUqvvfaa/ud//kcvvPBCkzlLly7V0aNHlZaWphtvvDE4/sILL2jGjBlqaGhQVlaWhg4dqurqam3YsEFvvvmm5syZo8cffzzscZ977jn94Q9/0LXXXqsbb7xRn3/+uRITE1VQUKDt27fr/fff11VXXRXyGcYf/OAHEf1Mv/zlLzVv3jxJUlZWloYPH65AIKC//e1v+s1vfqMRI0YEP9vo9/s1ceJE+Xw+XX755Ro8eLBOnDih7du3a968eVq0aJG2bNmivn37RriiZ7dkyRJNmjRJp06d0oABAzRq1CgFAgG98847mjhxotatW6fi4uKQbfx+v4YPH66PP/5YF1xwgW6++WY1NDRo4cKFWr16tQYOHBj2WCdOnNCIESP07rvv6rzzztMNN9ygLl26aPXq1Vq5cqVGjRrVbJ2tyfZMqampKigo0O7du7V582ZdeumlEefYnBtvvFFer1dr1qxRSkpKyO9kcnJyyNxf/epXeu6555SQkKBrr71Ww4cP14EDB7RixQq99tpr+q//+i/dfffdYY/zi1/8Qi+99JKGDRum0aNHa8+ePSHNNQCHMAAcbfny5UaSufDCC01dXV2T10eOHGkkmV//+tfBsdWrVxuXy2WSk5PNxo0bQ+Z/8MEHplevXkaS2bBhQ8hrmZmZRpJJTEw0K1asCFtPUVGRkWSKioqarTkvL89IMuvXrw8Zf/HFF4M/y7p165ps984775gDBw4En9fW1poVK1Y0+blPnz5tCgsLjSQzatSoJvspKCgwkkxJSUmzNX7XBx98YDwej/F6vebPf/5zyGv79u0zgwYNMpLMK6+8EvLaHXfcYSSZ4cOHm6NHjwbHjxw5YoYOHWokha3l4YcfNpLMFVdcYT7//PPg+MmTJ4P7DLfOLc32bEpKSowkU1BQEPb15vJs7ndh/fr1RpLJy8tr9pj/+Z//aSSZvn37mvfffz/ktY0bN5rzzz/fuN1u89FHH4W81rguSUlJpry8PNIfEYBN0agCDvfNN9+Y1NRUI8ksWbIk5LX9+/ebhIQEI8l8/PHHwfHGBul///d/w+5z8eLFRpK5/fbbQ8YbG9XJkyc3W09LG9VvvvnG9OjRw0hq0gi2VFpamklISDC1tbUh4y1pVMeOHWskmWeffTbs61u3bjWSTHZ2dnDswIEDJiEhwbhcLlNZWdlkm23btoVtVL/++mtz3nnnGUlmzZo1Tbb74osvTNeuXcOuc0uzPZt4N6r19fUmLS3NSDL/93//F3bOM888YySZf/3Xfw0Zb1zP3/zmN5H8aABsjj/9Aw7XqVMnFRQU6Omnn1ZxcbHuuOOO4GslJSVqaGhQXl5e8M/fNTU12rp1q7p06aJbbrkl7D4b/7S+ZcuWsK+feYxoqaio0OHDh5WcnKzbbrvN0rbvv/++ysrKtHfvXp04cUINDQ2SpL///e9qaGjQ7t27dfXVV7e4toaGBr3xxhuSpLFjx4adc+211+q8887Ttm3bdOrUKXm9Xm3atEkNDQ3Kzs4Oe//arKwsDR48uMnniysqKnT8+HElJyfrhhtuaLJdjx499KMf/UgrVqwIGY9Gtnawbds2ff7557r00kuVnZ0ddk5b/I4CiD8aVaAdmDx5sp5++mm9+eab+uyzz3TxxRfLGBO8ifyZX6Lau3evjDE6efKkPB7PWfd7+PDhsOPhvvTSWvv375ck9e/fP+LPEp44cUITJ07UsmXLzjqvtra2VbUdOXIkuI/09PSI5l988cX69NNPJUmXXHJJs3MvueSSJo1q43ZnW+dwr0UjWzvYs2ePJOmTTz455+9CPH9HAcQfjSrQDvTr10/Dhw/XW2+9pT/+8Y8qLCzU+vXrtW/fPvl8vpB3lxrfbTzvvPN0++23t+h4Xbp0iUrdrVVYWKhly5ZpwIABeuqppzRkyBAlJyfL7XZLkoYNG6by8nIZY1p1nMY1kxT8BvzZnKtJjNTZmrRwr0UjWzto/DlSU1OVn59/1rnf/QJWI7v8jgJoHRpVoJ2455579NZbb6mkpESFhYXBb5+PGzcu5D/aje8IulwuFRcXKyHBHnepa7xZ/EcffSRjTETvqi5evFiS9Oqrr2rw4MFNXv/444+jUltycrK6dOmikydP6tlnn222Ofquiy++WJLOelukcK+1dDu7ZmtV489x4YUX2v5/LQsgtpx5FQPQxE9/+lMlJSXp448/1uuvv66lS5dKanrv1LS0NA0ePFjHjh3T6tWro15H47uZf//73y1td+211yo5OVmHDx/W8uXLI9qm8T6kmZmZTV5bs2aNampqLNXQnMTERP3oRz+S9I/mOBI//OEP5XK59N5772nXrl1NXn///ffD3v82OztbXbt21eHDh/WXv/ylyes1NTVau3Ztk/FYZxst5/odaXxnvKqqSpWVlfEsDYDN0KgC7UTXrl01fvx4Sd9+ZvXkyZMaNGiQhgwZ0mTu7373O0nS3Xffrddee63J68YYvfPOO3rzzTct19GrVy9JstxgdOrUSY8++qgkaerUqdq0aVOTOe+++27w85uSdPnll0tS8L6rjf72t79p2rRplo5/LkVFRXK73XrkkUf0yiuvhHwcoNGHH34Y/AeC9O27xLfddpsaGhr0L//yLyGflf3qq690//33h/1YQteuXTVlyhRJ0sMPPyy/3x98ra6uTg8++KBOnDgRts5YZhstjb8jH3/8sb755psmr3fu3FlFRUUyxui2227TX//61yZz6uvrtW7dOr399tsxrxdA26FRBdqRxndPG79g8t13UxvdcssteuGFF/Tll1/qxz/+sS677DLdfPPNmjBhgm644Qalpqbqn/7pn7Ru3TrLNeTn5+t73/ueli9frh/84Ae6++67NWXKlHP+b14lafr06Zo2bZpqamqUl5ena665RuPHj9fo0aN16aWXKicnR7t37w7OLyoqksvl0uzZszV48GCNHz9e119/vQYNGqQ+ffpo2LBhlutvzjXXXBP8v0/dddddyszMVH5+vn7+859r1KhRSk9P16BBg5q84zp//nxdeuml2rBhgy655BLdfvvt+slPfqI+ffrI7/frxz/+cdjjPfnkk8rOztaHH36ovn37asyYMRo7dqz69OmjsrKy4GdlG9+dbBTLbKMlIyND1157rb744gsNGjRIP//5zzVlyhTNmjUrOOfBBx/UI488oo8//ljDhw/XlVdeqVtvvVXjx4/XiBEjlJycrOuvv17bt29vs58DQBy02Y2xAMRE443n3W63qampOevcHTt2mKlTp5rLLrvMeL1e07VrV9OnTx+Tn59vXnzxRfPZZ5+FzG+8j+revXvPut9NmzaZkSNHmgsuuCB4H9cz78HZ3H03G73xxhtmzJgxJiUlxXTu3Nn06NHD5OTkmDlz5pgjR440Odb1119vkpOTTdeuXc2VV15pnnzySVNXV9fscVpyH9VGe/fuNQ8//LC58sorzfe+9z3j9XpNZmamue6668xTTz1ldu/e3WSbmpoa84tf/ML06tXLuN1u06tXLzNt2jRz+PDhs9Zy7Ngx8+tf/9r06dPHuN1uk5qaaiZOnGj2799vJk+ebCSZl19+OWydVrM9m2jfR9WYb+/x+7Of/cz07NnTdOrUyUgymZmZTeZt3rzZTJgwwWRmZhqPx2POP/98069fP3Prrbea//7v/zZffvllyHz9//uoAmgfXMa08uuwAIC4+uabb3TllVfqo48+UkVFha655pq2LgkAYoI//QOATVVUVDT5LOzx48f14IMP6qOPPtLgwYNpUgG0a7yjCgA21bt3b3399dcaNGiQLrroIn3xxRfavn27vvzyS3Xv3l1/+ctfWvV/3AIAu6NRBQCbevHFF7Vs2TLt2rVLX331lRISEpSZmakbbrhBv/rVryL6v2QBgJPRqAIAAMCW+IwqAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJYsN6qbNm3SLbfcorS0NLlcLi1fvvyc22zYsEHXXHONPB6P+vbtq9LS0haUCivIyRnIyRnIyTnIyhnICZGy3KieOHFCV111lebPnx/R/L1792r06NEaMWKEtm/froceekhTpkzRmjVrLBeLyJGTM5CTM5CTc5CVM5ATItWq+6i6XC4tW7ZMt956a7Nz/u3f/k0rV67Uhx9+GBwbN26cjh49qtWrV4fdpq6uTnV1dcHnDQ0N+vLLL3XhhRfK5XK1tNwOy+fzaeHChbr55pvDvm6MUWFhoTZt2kRObehcOUnS448/rjfeeEM7d+5UQsK3/84kp/iKVU4SWUUb1z5nIKf2wxijY8eOKS0tLXjti8ZOW0ySWbZs2VnnDB8+3EyfPj1krLi42CQlJTW7TVFRkZHEI86Pe+65h5wc8jh48CA5OeBhJSeyarsH1z5nPMjJOY8zr32t1UkxVl1drZSUlJCxlJQU1dbW6uTJk+rSpUuTbQoLCzVjxozg80AgoIyMDB08eFBJSUmxLrndOde/Vmtra5Wenq6LL744ZJyc4iuSd+qysrK0d+9enX/++cExcoqvWOUkkVW0ce1zBnJqPxqzOvPa11oxb1RbwuPxyOPxNBlPSkril6uFunbtGvW1I6foO1dOjX9KsfJnK3KKvljkJJFVLHDtcwZyal+i+dGKmN+eKjU1VX6/P2TM7/crKSmp2XcV0DYOHz4c8pyc7Oe7f52QyMmOyMlZuPY5Azl1TDFvVHNzc1VWVhYytnbtWuXm5sb60LBo48aNIc/JyX6GDBnSZIyc7IecnIVrnzOQU8dkuVE9fvy4tm/fru3bt0v69pYR27dv14EDByR9+5mQSZMmBedPmzZNe/bs0cyZM7Vr1y699NJLWrx4sR5++OHo/AQIy2pOkrRv3z5yijOrOU2ePFmSNHv2bHKKI3JyDq59zkBOiJjVb1+tX78+7De8CgoKjDHGFBQUmLy8vCbbZGVlGbfbbfr06WNKSkosHTMQCBhJJhAIWC23w7KSU+P6vv766+QUZ1bPp8Y1HjRoEDnFUVvkdOZ+yCpyXPucgZzap1iscavuoxovtbW18vl8CgQCfAA6BqK1vuQUe9FYY3KKPc4pZyAnZyAn54jFGsf8M6oAAABAS9CoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAttahRnT9/vnr37i2v16uhQ4dq69atzc4tLS2Vy+UKeXi93hYXjMhZyUmSfD4fObURsnIGcnIGcnIGckIkLDeqr776qmbMmKGioiK99957uuqqq5Sfn68vvvii2W2SkpJ06NCh4GP//v2tKhrnRk7OQVbOQE7OQE7OQE6ImLEoJyfHPPDAA8Hn9fX1Ji0tzcydOzfs/JKSEuPz+aweJkQgEDCSTCAQaNV+OhIrOTWuLzm1jXhnRU4twznlDOTkDOTUPsVijS29o3r69GlVVFRo5MiRwbGEhASNHDlS5eXlzW53/PhxZWZmKj09XWPGjFFlZeVZj1NXV6fa2tqQByJHTs4Rj6zIqfU4p5yBnJyBnGCFpUa1pqZG9fX1SklJCRlPSUlRdXV12G369++v4uJirVixQgsWLFBDQ4OGDRumTz/9tNnjzJ07Vz6fL/hIT0+3UmaH15KcpG8/L0RO8RWPrMip9TinnIGcnIGcYEXMv/Wfm5urSZMmKSsrS3l5eVq6dKl69Oihl19+udltCgsLFQgEgo+DBw/GukxIGj9+PDk5hJWsyKntcE45Azk5Azl1TJ2sTE5OTlZiYqL8fn/IuN/vV2pqakT76Ny5s66++mrt3r272Tkej0cej8dKaTgDOTlHPLIip9bjnHIGcnIGcoIVlt5Rdbvdys7OVllZWXCsoaFBZWVlys3NjWgf9fX12rFjh3r27GmtUkSMnJyDrJyBnJyBnJyBnGCJ1W9fLVq0yHg8HlNaWmqqqqrM1KlTTbdu3Ux1dbUxxpiJEyeaWbNmBefPmTPHrFmzxnzyySemoqLCjBs3zni9XlNZWRnxMfmmnnVWcmpc36VLl5JTG4h3VuTUMpxTzkBOzkBO7VMs1tjSn/4laezYsTp8+LAef/xxVVdXKysrS6tXrw5+KPrAgQNKSPjHG7VfffWV7r33XlVXV+uCCy5Qdna2tmzZoiuuuKJ1HTbOympOkjR9+nT5/X5yijOycgZycgZycgZyQqRcxhjT1kWcS21trXw+nwKBgJKSktq6nHYnWutLTrEXjTUmp9jjnHIGcnIGcnKOWKxxzL/1DwAAALQEjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2FKLGtX58+erd+/e8nq9Gjp0qLZu3XrW+UuWLNGAAQPk9Xo1aNAgrVq1qkXFwhqrOS1btoyc2ghZOQM5OQM5OQM5ISLGokWLFhm3222Ki4tNZWWluffee023bt2M3+8PO3/z5s0mMTHRPPPMM6aqqso89thjpnPnzmbHjh0RHzMQCBhJJhAIWC23w7KSU+P6klPbiHdW5NQynFPOQE7OQE7tUyzW2HKjmpOTYx544IHg8/r6epOWlmbmzp0bdv6dd95pRo8eHTI2dOhQc99990V8TH65rLOSU+P65ufnh4yTU3zEOytyahnOKWcgJ2cgp/YpFmvcycq7r6dPn1ZFRYUKCwuDYwkJCRo5cqTKy8vDblNeXq4ZM2aEjOXn52v58uXNHqeurk51dXXB54FAQJJUW1trpdwOqzGn6dOnh6xZXl6e3nrrLd1///0h8xvn5OXlhYyTU+zFIytyaj3OKWcgJ2cgp/arcW2NMVHbp6VGtaamRvX19UpJSQkZT0lJ0a5du8JuU11dHXZ+dXV1s8eZO3eu5syZ02Q8PT3dSrkd3oQJE8KO+3y+sONdunQJeU5O8RPLrMgpejinnIGcnIGc2q8jR440m6NVlhrVeCksLAx5F/bo0aPKzMzUgQMHovaDx1Jtba3S09N18OBBJSUlxf34hw4d0oABA7R27Vrl5OQEx2fPnq3Nmzdr3bp1IfMDgYAyMjJ03nnnWToOObVePLJyek5S22fFORUZciKnSJBTZNo6p5ZozKp79+5R26elRjU5OVmJiYny+/0h436/X6mpqWG3SU1NtTRfkjwejzweT5Nxn8/nmLAkKSkpqU3q9Xq9SkxM1PHjx0OOf/ToUV188cXN1lRTUxPynJxiLx5ZtZecJM4ppyAnZyAnZ2jL/0a1VEJC9O5+amlPbrdb2dnZKisrC441NDSorKxMubm5YbfJzc0NmS9Ja9eubXY+Wq8lOUnSxo0bQ56TU+yRlTOQkzOQkzOQEyyx+u2rRYsWGY/HY0pLS01VVZWZOnWq6datm6murjbGGDNx4kQza9as4PzNmzebTp06mWeffdbs3LnTFBUVtftbStihXis5NdZLTm0j3lnZ5ee2wg41c06dmx3qJadzs0O95HRuTqvXGJvcnsoYY+bNm2cyMjKM2+02OTk55u233w6+lpeXZwoKCkLmL1682PTr18+43W4zcOBAs3LlSkvHO3XqlCkqKjKnTp1qSblxZ5d6I82psd6FCxeSUxuJZ1Z2+rkjZZeaOafOzi71ktPZ2aVecjo7p9VrTGxqdhkTxXsIAAAAAFESvU+7AgAAAFFEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLtmlU58+fr969e8vr9Wro0KHaunXrWecvWbJEAwYMkNfr1aBBg7Rq1ao4VfotK/WWlpbK5XKFPLxeb9xq3bRpk2655RalpaXJ5XJp+fLl59xmw4YNuuaaa+TxeNS3b1+VlpZKIqdY6sg5Sc7JKpo5Sc7Lyik5SR37nCIncoq2aF/7Iha1G121wqJFi4zb7TbFxcWmsrLS3HvvvaZbt27G7/eHnb9582aTmJhonnnmGVNVVWUee+wxyzf+jWe9JSUlJikpyRw6dCj4aLypcTysWrXKPProo2bp0qVGklm2bNlZ5+/Zs8d07drVzJgxw1RVVZl58+aZxMREU1hYSE4x1FFzMsZZWUUrp9WrV3Pti7GOek6REznFQjSvfVbYolHNyckxDzzwQPB5fX29SUtLM3Pnzg07/8477zSjR48OGRs6dKi57777YlpnI6v1lpSUGJ/PF5faziWSX66ZM2eagQMHhoyNHTvW+Hw+coqTjpSTMc7NqjU55efnc+2Lo450TpHTt8gpdlp77bOizf/0f/r0aVVUVGjkyJHBsYSEBI0cOVLl5eVhtykvLw+ZL0n5+fnNzo+mltQrScePH1dmZqbS09M1ZswYVVZWxrzWlgq3vtdff70CgQA52Uh7yElq/1k1t75btmzh2mcz7eGcIidysotorW+bN6o1NTWqr69XSkpKyHhKSoqqq6vDblNdXW1pfjS1pN7+/furuLhYK1as0IIFC9TQ0KBhw4bp008/jXm9LRFufRs/B9OtW7eQcXJqO+0hJ6n9Z9Xc+h47doxrn820h3OKnLqFjJNT22lufWtra3Xy5MmI99Mp2oWhqdzcXOXm5gafDxs2TJdffrlefvll/fa3v23DynAmcnIOsnIGcnIGcnKGjpqT5XdUo/2tr+TkZCUmJsrv94ds4/f7lZqaGnZ/qampluZHU0vq/a7OnTvr6quv1u7du2NRoqTW5bR79249/fTTITmdOnVKknT06NGQbcipdTp6TpIzsop2Tn6/X+effz7Xvhjo6OcUOR0N2Yac2k5z65uUlKQuXbpEvB/LjeqJEyd01VVXaf78+RHN37t3r0aPHq0RI0Zo+/bteuihhzRlyhStWbNGkuR2u5Wdna2ysrLgNg0NDSorKwv5l8OZcnNzQ+ZL0tq1a5udH00tqfe76uvrtWPHDvXs2TNWZbYqp8mTJ6tr164hOa1fv14+n4+coqyj5yQ5I6to57R27VoNGzaMa18MdPRzipzIyS6itb6W//R/00036aabbop4/n/8x3/okksu0XPPPSdJuvzyy/XXv/5Vv//975Wfny9JmjFjhgoKCnTttdcqJydHzz33nI4fP6477rhDtbW1mjJlirp3766nn35aLpdLU6ZM0U033aQnn3xS+fn5+vOf/6x3331Xzz//vGpra63+SJZNmzZN06ZN08CBA5Wdna2XXnoppN6pU6cqLS1NTzzxhCTpqaee0pAhQ9SnTx8FAgG9+OKL2rdvn8aNGxezer///e/r+9//fvB5VVWVkpOTJUk7d+5UcnKyLrjgAqWnp6uoqEirVq0K5rR371796U9/0mWXXabf/e53+uSTT7R48WLNnDlTzz//PDlFkZWcnnjiCa1atUq9evXSv//7v2v//v3tIifJ/lnFIqeVK1fq6NGjXPuijGsfOZFTbBw/flx79uwJPg937fvss8/03HPPKS0tTdOmTdMf/vAHzZw5U5MnT9a6deuC1z5LrN6S4EyK4PYEw4cPN9OnTw8ZKy4uNklJSSFj8+bNMxkZGcbtdpu0tDQjiUecH/fcc08wj/Xr15v09HQjyfTp08eUlJSQk40eBw8eJCcHPM6VE1nZ48G1zxkPcnLO48xrX1ZWlnG73U2ufZFyGWOMWsjlcmnZsmW69dZbm53Tr18/3X333SosLAyOrVq1SqNHj9bXX38d9nMKdXV1qqurCz4PBALKyMjQwYMHlZSU1NJyOyyfz6eFCxfq5ptvDvt6bW2t0tPT9fjjj2vOnDnBcXKKr3PlJElZWVnau3evjh49Kp/PJ4mc4i1WOUlkFW1c+5yBnNqPxqzOvPa1li2/9e/xeOTxeJqMJyUl8cvVQl27do362pFT9J0rp4SEbz9W7nK5It4nOUVfLHKSyCoWuPY5Azm1L1avfWcT8/uoRutbX4i9w4cPhzwnJ/v57j3pJHKyI3JyFq59zkBOHVPMG9W2/kYxIrdx48aQ5+RkP0OGDGkyRk72Q07OwrXPGcipY7LcqB4/flzbt2/X9u3bJX17y4jt27frwIEDkqTCwkJNmjQpOH/atGnas2ePZs6cqV27dumll17S4sWL9fDDD0fnJ0BYVnOSpH379pFTnFnNafLkyZKk2bNnk1MckZNzcO1zBnJCxKx++2r9+vVhv+FVUFBgjDGmoKDA5OXlNdmmNd/6CgQCRpIJBAJWy+2wrOTUuL6vv/46OcWZ1fOpcY0HDRpETnHUFjmduR+yihzXPmcgp/YpFmvcqm/9x0ttba18Pp8CgQAfgI6BaK0vOcVeNNaYnGKPc8oZyMkZyMk5YrHGMf+MKgAAANASNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEstalTnz5+v3r17y+v1aujQodq6dWuzc0tLS+VyuUIeXq+3xQUjclZykiSfz0dObYSsnIGcnIGcnIGcEAnLjeqrr76qGTNmqKioSO+9956uuuoq5efn64svvmh2m6SkJB06dCj42L9/f6uKxrmRk3OQlTOQkzOQkzOQEyJmLMrJyTEPPPBA8Hl9fb1JS0szc+fODTu/pKTE+Hw+q4cJEQgEjCQTCARatZ+OxEpOjetLTm0j3lmRU8twTjkDOTkDObVPsVhjS++onj59WhUVFRo5cmRwLCEhQSNHjlR5eXmz2x0/flyZmZlKT0/XmDFjVFlZedbj1NXVqba2NuSByJGTc8QjK3JqPc4pZyAnZyAnWGGpUa2pqVF9fb1SUlJCxlNSUlRdXR12m/79+6u4uFgrVqzQggUL1NDQoGHDhunTTz9t9jhz586Vz+cLPtLT062U2eG1JCfp288LkVN8xSMrcmo9zilnICdnICdYEfNv/efm5mrSpEnKyspSXl6eli5dqh49eujll19udpvCwkIFAoHg4+DBg7EuE5LGjx9PTg5hJStyajucU85ATs5ATh1TJyuTk5OTlZiYKL/fHzLu9/uVmpoa0T46d+6sq6++Wrt37252jsfjkcfjsVIazkBOzhGPrMip9TinnIGcnIGcYIWld1Tdbreys7NVVlYWHGtoaFBZWZlyc3Mj2kd9fb127Nihnj17WqsUESMn5yArZyAnZyAnZyAnWGL121eLFi0yHo/HlJaWmqqqKjN16lTTrVs3U11dbYwxZuLEiWbWrFnB+XPmzDFr1qwxn3zyiamoqDDjxo0zXq/XVFZWRnxMvqlnnZWcGtd36dKl5NQG4p0VObUM55QzkJMzkFP7FIs1tvSnf0kaO3asDh8+rMcff1zV1dXKysrS6tWrgx+KPnDggBIS/vFG7VdffaV7771X1dXVuuCCC5Sdna0tW7boiiuuaF2HjbOympMkTZ8+XX6/n5zijKycgZycgZycgZwQKZcxxrR1EedSW1srn8+nQCCgpKSkti6n3YnW+pJT7EVjjckp9jinnIGcnIGcnCMWaxzzb/0DAAAALUGjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC2RKMKAAAAW6JRBQAAgC3RqAIAAMCWaFQBAABgSzSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADYEo0qAAAAbIlGFQAAALZEowoAAABbolEFAACALdGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2BKNKgAAAGyJRhUAAAC21KJGdf78+erdu7e8Xq+GDh2qrVu3nnX+kiVLNGDAAHm9Xg0aNEirVq1qUbGwxmpOy5YtI6c2QlbOQE7OQE7OQE6IiLFo0aJFxu12m+LiYlNZWWnuvfde061bN+P3+8PO37x5s0lMTDTPPPOMqaqqMo899pjp3Lmz2bFjR8THDAQCRpIJBAJWy+2wrOTUuL7k1DbinRU5tQznlDOQkzOQU/sUizW23Kjm5OSYBx54IPi8vr7epKWlmblz54adf+edd5rRo0eHjA0dOtTcd999ER+TXy7rrOTUuL75+fkh4+QUH/HOipxahnPKGcjJGcipfYrFGney8u7r6dOnVVFRocLCwuBYQkKCRo4cqfLy8rDblJeXa8aMGSFj+fn5Wr58ebPHqaurU11dXfB5IBCQJNXW1lopt8NqzGn69Okha5aXl6e33npL999/f8j8xjl5eXkh4+QUe/HIipxaj3PKGcjJGcip/WpcW2NM1PZpqVGtqalRfX29UlJSQsZTUlK0a9eusNtUV1eHnV9dXd3scebOnas5c+Y0GU9PT7dSboc3YcKEsOM+ny/seJcuXUKek1P8xDIrcooezilnICdnIKf268iRI83maJWlRjVeCgsLQ96FPXr0qDIzM3XgwIGo/eCxVFtbq/T0dB08eFBJSUlxP/6hQ4c0YMAArV27Vjk5OcHx2bNna/PmzVq3bl3I/EAgoIyMDJ133nmWjkNOrRePrJyek9T2WXFORYacyCkS5BSZts6pJRqz6t69e9T2aalRTU5OVmJiovx+f8i43+9Xampq2G1SU1MtzZckj8cjj8fTZNzn8zkmLElKSkpqk3q9Xq8SExN1/PjxkOMfPXpUF198cbM11dTUhDwnp9iLR1btJSeJc8opyMkZyMkZ2vK/US2VkBC9u59a2pPb7VZ2drbKysqCYw0NDSorK1Nubm7YbXJzc0PmS9LatWubnY/Wa0lOkrRx48aQ5+QUe2TlDOTkDOTkDOQES6x++2rRokXG4/GY0tJSU1VVZaZOnWq6detmqqurjTHGTJw40cyaNSs4f/PmzaZTp07m2WefNTt37jRFRUXt/pYSdqjXSk6N9ZJT24h3Vnb5ua2wQ82cU+dmh3rJ6dzsUC85nZvT6jXGJrenMsaYefPmmYyMDON2u01OTo55++23g6/l5eWZgoKCkPmLFy82/fr1M2632wwcONCsXLnS0vFOnTplioqKzKlTp1pSbtzZpd5Ic2qsd+HCheTURuKZlZ1+7kjZpWbOqbOzS73kdHZ2qZeczs5p9RoTm5pdxkTxHgIAAABAlETv064AAABAFNGoAgAAwJZoVAEAAGBLNKoAAACwJRpVAAAA2JJtGtX58+erd+/e8nq9Gjp0qLZu3XrW+UuWLNGAAQPk9Xo1aNAgrVq1Kk6VfstKvaWlpXK5XCEPr9cbt1o3bdqkW265RWlpaXK5XFq+fPk5t9mwYYOuueYaeTwe9e3bV6WlpZLIKZY6ck6Sc7KKZk6S87JySk5Sxz6nyImcoi3a176IRe1GV62waNEi43a7TXFxsamsrDT33nuv6datm/H7/WHnb9682SQmJppnnnnGVFVVmccee8zyjX/jWW9JSYlJSkoyhw4dCj4ab2ocD6tWrTKPPvqoWbp0qZFkli1bdtb5e/bsMV27djUzZswwVVVVZt68eSYxMdEUFhaSUwx11JyMcVZW0cpp9erVXPtirKOeU+RETrEQzWufFbZoVHNycswDDzwQfF5fX2/S0tLM3Llzw86/8847zejRo0PGhg4dau67776Y1tnIar0lJSXG5/PFpbZzieSXa+bMmWbgwIEhY2PHjjU+n4+c4qQj5WSMc7NqTU75+flc++KoI51T5PQtcoqd1l77rGjzP/2fPn1aFRUVGjlyZHAsISFBI0eOVHl5edhtysvLQ+ZLUn5+frPzo6kl9UrS8ePHlZmZqfT0dI0ZM0aVlZUxr7Wlwq3v9ddfr0AgQE420h5yktp/Vs2t75YtW7j22Ux7OKfIiZzsIlrr2+aNak1Njerr65WSkhIynpKSourq6rDbVFdXW5ofTS2pt3///iouLtaKFSu0YMECNTQ0aNiwYfr0009jXm9LhFvfxs/BdOvWLWScnNpOe8hJav9ZNbe+x44d49pnM+3hnCKnbiHj5NR2mlvf2tpanTx5MuL9dIp2YWgqNzdXubm5wefDhg3T5Zdfrpdfflm//e1v27AynImcnIOsnIGcnIGcnKGj5tTm76gmJycrMTFRfr8/ZNzv9ys1NTXsNqmpqZbmR1NL6v2uzp076+qrr9bu3btjUWKrhVvfU6dOSZKOHj0aMk5Obac95CS1/6yaW9/zzz+fa5/NtIdzipyOhoyTU9tpbn2TkpLUpUuXiPfT5o2q2+1Wdna2ysrKgmMNDQ0qKysL+ZfDmXJzc0PmS9LatWubnR9NLan3u+rr67Vjxw717NkzVmW2Srj1Xb9+vXw+HznZSHvISWr/WTW3vsOGDePaZzPt4ZwiJ3Kyi6itr9VvesXCokWLjMfjMaWlpaaqqspMnTrVdOvWLXjbhYkTJ5pZs2YF52/evNl06tTJPPvss2bnzp2mqKgo7reUsFLvnDlzzJo1a8wnn3xiKioqzLhx44zX6zWVlZVxqffYsWNm27ZtZtu2bUaSef755822bdvM/v37jTHGzJo1y0ycODE4v/GWEo888ojZuXOnmT9/fvDWH+QUOx01J2OclVW0cmq8PZWTsnJSTsZ03HOKnMgpFqJ57bPCFo2qMcbMmzfPZGRkGLfbbXJycszbb78dfC0vL88UFBSEzF+8eLHp16+fcbvdZuDAgWblypW2rfehhx4Kzk1JSTGjRo0y7733XtxqXb9+vZHU5NFYY0FBgcnLy2uyTVZWlnG73aZPnz6mpKTEGENOsdSRczLGOVlFMydjnJeVU3IypmOfU+RETtEW7WtfpFzGGGPtPVgAAAAg9tr8M6oAAABAODSqAAAAsCUaVQAAANgSjSoAAABsiUYVAAAAtkSjCgAAAFuiUQUAAIAt0agCAADAlmhUAQAAYEs0qgAAALAlGlUAAADY0v8DdZNCOFI3tuwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convolution(torch.tensor([[-1.,0.,1.],[-1.,0.,1.],[-1.,0.,1.]]),'Vertical edge filter')   # 수직선 판별하기\n",
    "plot_convolution(torch.tensor([[-1.,-1.,-1.],[0.,0.,0.],[1.,1.,1.]]),'Horizontal edge filter') # 수평선 판별하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First filter is called a **vertical edge filter**, and it is defined by the following matrix:\n",
    "$$\n",
    "\\left(\n",
    "    \\begin{matrix}\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "    \\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "When this filter goes over relatively uniform pixel field, all values add up to 0. However, when it encounters a vertical edge in the image, high spike value is generated. That's why in the images above you can see vertical edges represented by high and low values, while horizontal edges are averaged out.\n",
    "\n",
    "An opposite thing happens when we apply horizontal edge filter - horizontal lines are amplified, and vertical are averaged out.\n",
    "\n",
    "In classical computer vision, multiple filters were applied to the image to generate features, which then were used by machine learning algorithm to build a classifier. However, in deep learning we construct networks that **learn** best convolutional filters to solve classification problem.\n",
    "\n",
    "To do that, we introduce **convolutional layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covolutional layers\n",
    "\n",
    "Convolutional layers are defined using `nn.Conv2d` construction. We need to specify the following:\n",
    "* `in_channels` - number of input channels. In our case we are dealing with a grayscale image, thus number of input channels is 1.\n",
    "* `out_channels` - number of filters to use. We will use 9 different filters, which will give the network plenty of opportunities to explore which filters work best for our scenario.\n",
    "* `kernel_size` is the size of the sliding window. Usually 3x3 or 5x5 filters are used.\n",
    "\n",
    "Simplest CNN will contain one convolutional layer. Given the input size 28x28, after applying nine 5x5 filters we will end up with a tensor of 9x24x24 (the spatial size is smaller, because there are only 24 positions where a sliding interval of length 5 can fit into 28 pixels).\n",
    "\n",
    "After convolution, we flatten 9x24x24 tensor into one vector of size 5184, and then add linear layer, to produce 10 classes. We also use `relu` activation function in between layers. \n",
    "\n",
    "The Rectified Linear Unit (ReLU) activation function is one of the most commonly used activation functions in neural networks, especially in deep learning models. The function is defined mathematically as:\n",
    "\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "Here’s what this means:\n",
    "\n",
    "If x is greater than 0, the function returns x.\n",
    "If x is less than or equal to 0, the function returns 0.\n",
    "\n",
    "Properties of ReLU\n",
    "Non-linear: While it looks like a linear function, ReLU introduces a non-linearity (a simple threshold at 0), which allows models to learn more complex patterns.\n",
    "Computationally Efficient: It is very efficient to compute as it only requires checking if the input is positive or not.\n",
    "Sparse Activation: In practice, ReLU results in sparse activations; i.e., only a subset of neurons in a layer are active at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 9, 24, 24]             234\n",
      "           Flatten-2                 [-1, 5184]               0\n",
      "            Linear-3                   [-1, 10]          51,850\n",
      "================================================================\n",
      "Total params: 52,084\n",
      "Trainable params: 52,084\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.08\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 0.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 파이토치(PyTorch)를 사용하여 간단한 신경망 모델을 정의하고, 그 구조를 요약해서 출력하는 과정을 설명\n",
    "\n",
    "import torch.nn as nn # 파이토치의 신경망 모듈(nn)을 임포트\n",
    "%pip install torchsummary # 신경망 모델의 요약 정보를 제공하는 torchsummary 라이브러리를 설치\n",
    "from torchsummary import summary # 설치된 torchsummary에서 summary 함수를 임포트\n",
    "\n",
    "class OneConv(nn.Module): # nn.Module을 상속받는 OneConv라는 새로운 클래스를 정의하는데 nn.Module은 파이토치에서 모든 신경망 모듈의 기본 클래스\n",
    "    def __init__(self): # 클래스의 생성자로, OneConv 객체가 생성될 때 초기화 과정을 정의\n",
    "        super(OneConv, self).__init__() # 상위 클래스인 nn.Module의 생성자를 호출하여 클래스를 적절히 초기화\n",
    "        self.conv = nn.Conv2d(in_channels=1,out_channels=9,kernel_size=(5,5)) # 입력 채널이 1개, 출력 채널이 9개인 2D 컨볼루션 레이어를 정의하고, 이를 객체의 conv 속성으로 할당\n",
    "        self.flatten = nn.Flatten() # 다차원 입력을 1차원 배열로 변환하는 Flatten 레이어를 정의하고, flatten 속성으로 할당\n",
    "        self.fc = nn.Linear(5184,10) # 5,184개의 입력 특성을 10개의 출력 특성으로 변환하는 완전 연결 레이어(또는 선형 레이어)를 정의하고, fc 속성으로 할당\n",
    "\n",
    "    def forward(self, x): # 모델의 순전파를 정의하는 메서드로 입력 텐서 x가 모델을 통과하는 과정을 설명\n",
    "        if x.dim() == 5 and x.size(2) == 1:  # 입력 x의 차원이 예상치 못한 추가 차원을 포함하는 경우, 해당 차원을 제거\n",
    "            x = x.squeeze(2) # 차원수 줄이기\n",
    "        x = nn.functional.relu(self.conv(x)) # 정의된 컨볼루션 레이어를 입력 x에 적용한 후 ReLU 활성화 함수를 적용\n",
    "        x = self.flatten(x) # ReLU의 출력을 flatten 레이어를 통해 평탄화\n",
    "        x = nn.functional.log_softmax(self.fc(x),dim=1) # 평탄화된 벡터를 fc 레이어에 적용하고, 결과를 로그 소프트맥스 함수를 통해 처리하는데 이 함수는 다중 클래스 분류 문제에 대한 확률을 계산할 때 사용\n",
    "\n",
    "        return x  # The final processed tensor x, which contains the log probabilities of the classes, is returned from the forward method. This output can be used by a loss function during training to compute the error and update the model weights\n",
    "\n",
    "# Create an instance of the network\n",
    "net = OneConv() # OneConv 클래스의 인스턴스를 생성\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(net,input_size=(1,1,28,28)) # 생성된 모델 net의 요약 정보를 출력하는데 여기서 input_size는 모델이 기대하는 입력 데이터의 크기를 (채널 수, 높이, 너비) 형식으로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this network contains around 50k trainable parameters, compared to around 80k in fully-connected multi-layered networks. This allows us to achieve good results even on smaller datasets, because convolutional networks generalize much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch를 사용하여 신경망 모델을 학습하고 검증하는 과정을 구현\n",
    "\n",
    "import torch # torch, torch.optim, torch.nn.functional 모듈을 임포트하는데 PyTorch에서 모델을 구성하고 최적화하는 데 필요한 함수와 클래스를 제공\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, train_loader, test_loader, val_loader, epochs=5): # 함수를 정의하여 모델, 학습 데이터 로더, 테스트 데이터 로더, 그리고 에폭 수를 매개변수로 받음\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 사용 가능한 경우 CUDA를 사용하고, 그렇지 않으면 CPU를 사용하도록 설정\n",
    "    model.to(device) # 모델을 해당 장치로 이동\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam 최적화 도구를 사용하여 모델의 매개변수를 최적화하고, 학습률은 0.001로 설정\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 다중 클래스 분류를 위한 크로스 엔트로피 손실 함수를 사용\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "    # 에폭 수만큼 반복하면서 모델의 학습 및 평가를 수행하는데 학습 시에는 model.train()을 호출하여 모델을 학습 모드로 설정하고, 평가 시에는 model.eval()을 호출하여 평가 모드로 설정\n",
    "\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            model.train() # 학습모드\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "            for data, target in train_loader: # # 각 배치 데이터에 대해 데이터와 타겟을 장치로 이동시킨 후, 최적화 도구를 이용해 그래디언트를 초기화하고, 모델을 통해 예측을 수행\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                \n",
    "                if output is None:\n",
    "                    print(\"Warning: Model output is None.\")\n",
    "                    continue\n",
    "                \n",
    "                loss = criterion(output, target) # 손실을 계산하고, 역전파를 통해 그래디언트를 계산한 다음, 최적화 도구로 매개변수를 업데이트\n",
    "                if loss is None:\n",
    "                    print(\"Warning: Loss computation returned None.\")\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item() # 정확도와 손실을 기록하여 진행 상황을 모니터링\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                train_total += target.size(0)\n",
    "                train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            \n",
    "            model.eval() #평가모드\n",
    "            test_loss, test_correct, test_total = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    if output is None:\n",
    "                        print(\"Warning: Model output is None during evaluation.\")\n",
    "                        continue\n",
    "                    \n",
    "                    loss = criterion(output, target)\n",
    "                    if loss is None:\n",
    "                        print(\"Warning: Loss computation returned None during evaluation.\")\n",
    "                        continue\n",
    "                    \n",
    "                    test_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    test_total += target.size(0)\n",
    "                    test_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_acc = 100. * test_correct / test_total\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "            # Validation 과정 추가----------------------------------------------------------\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    if output is None:\n",
    "                        print(\"Warning: Model output is None during validation.\")\n",
    "                        continue\n",
    "                        \n",
    "                    loss = criterion(output, target)\n",
    "                    if loss is None:\n",
    "                        print(\"Warning: Loss computation returned None during validation.\")\n",
    "                        continue\n",
    "                        \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += (predicted == target).sum().item()\n",
    "                \n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_acc = 100. * val_correct / val_total\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "                \n",
    "    except Exception as e: # 에러가 발생할 경우 처리하는데 에러 발생 시 해당 에러 메시지를 출력하고 None을 반환\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return history # 학습과 테스트 과정에서의 손실과 정확도를 기록한 history 딕셔너리를 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 0.0042, Train Acc: 81.41%, Test Loss: 0.0035, Test Acc: 84.33%\n",
      "Epoch 1/5: Val Loss: 0.0035, Val Acc: 84.33%\n",
      "Epoch 2/5: Train Loss: 0.0030, Train Acc: 86.68%, Test Loss: 0.0032, Test Acc: 85.82%\n",
      "Epoch 2/5: Val Loss: 0.0032, Val Acc: 85.82%\n",
      "Epoch 3/5: Train Loss: 0.0027, Train Acc: 88.00%, Test Loss: 0.0031, Test Acc: 86.25%\n",
      "Epoch 3/5: Val Loss: 0.0031, Val Acc: 86.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 신경망 모델 net을 학습시키고 그 결과를 처리하는 과정을 보여줌\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train 함수를 호출하여 신경망 모델 net을 학습시키는데 학습 데이터 로더 train_loader, 테스트 데이터 로더 test_loader를 사용하고, 총 5 에폭(epoch) 동안 학습을 수행하고 학습 과정에서 계산된 손실과 정확도 같은 통계 정보를 hist 변수에 저장\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# train 함수에서 반환된 hist가 None인지 확인하는데 None은 학습 과정에서 오류가 발생했거나 예외가 처리되었음을 의미할 수 있음\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining did not return any history.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 만약 hist가 None이면, \"학습이 어떠한 기록도 반환하지 않았습니다.\"라는 메시지를 출력하는데 이는 학습 과정에서 문제가 발생했음을 사용자에게 알리는 역할\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Loss computation returned None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# 정확도와 손실을 기록하여 진행 상황을 모니터링\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 신경망 모델 net을 학습시키고 그 결과를 처리하는 과정을 보여줌\n",
    "\n",
    "# train 함수를 호출하여 신경망 모델 net을 학습시키는데 학습 데이터 로더 train_loader, 테스트 데이터 로더 test_loader를 사용하고, 총 5 에폭(epoch) 동안 학습을 수행하고 학습 과정에서 계산된 손실과 정확도 같은 통계 정보를 hist 변수에 저장\n",
    "hist = train(net,train_loader,test_loader,val_loader, epochs=5)\n",
    "\n",
    "if hist is None: # train 함수에서 반환된 hist가 None인지 확인하는데 None은 학습 과정에서 오류가 발생했거나 예외가 처리되었음을 의미할 수 있음\n",
    "    print(\"Training did not return any history.\") # 만약 hist가 None이면, \"학습이 어떠한 기록도 반환하지 않았습니다.\"라는 메시지를 출력하는데 이는 학습 과정에서 문제가 발생했음을 사용자에게 알리는 역할\n",
    "\n",
    "else: # 만약 hist가 None이 아니라 유효한 데이터가 있으면\n",
    "    plot_results(hist) # plot_results 함수를 호출하여 hist에 저장된 학습 결과(손실과 정확도 등)를 시각화하는데 이 함수는 일반적으로 matplotlib 같은 라이브러리를 사용하여 그래프를 그리고, 모델의 학습 진행 상황을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to achieve higher accuracy, and much faster, compared to the fully-connected networks.\n",
    "\n",
    "We can also visualize the weights of our trained convolutional layers, to try and make some more sense of what is going on:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
